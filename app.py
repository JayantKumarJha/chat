# -*- coding: utf-8 -*-
"""Chat and  PDF answering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1raHBzHh7KxOwrle-EMYxfaF973o11lsx
"""

# app.py
import os
import time
import io
import hashlib
from typing import Tuple, List

import streamlit as st
from PyPDF2 import PdfReader
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter

from transformers import pipeline
from huggingface_hub import login

# ---------------------------
# CONFIG
# ---------------------------
CALL_COOLDOWN_SECONDS = 30    # minimum seconds between model calls per user/session
PIPELINE_TTL_SECONDS = 300    # reload pipeline every 5 minutes (auto-refresh)
ANSWER_CACHE_TTL = 60 * 60    # cache identical Q&A outputs for 1 hour

MODEL_OPTIONS = {
    "small": "google/flan-t5-small",
    "base": "google/flan-t5-base",
    "large": "google/flan-t5-large",
}

DEFAULT_MODEL = "base"

# Maximum characters of context to include in prompt (to avoid huge prompts)
CONTEXT_CHAR_LIMIT = {
    "small": 1500,
    "base": 3500,
    "large": 8000,
}

# ---------------------------
# HELPERS: auth + pipeline
# ---------------------------
st.set_page_config(page_title="FLAN T5 Q&A (rate-limited, cached)", layout="wide")
st.title("ðŸ“˜ FLAN T5 Q&A â€” safe, cached, and rate-limited")

# attempt to login to Hugging Face using secrets (safe)
hf_token = None
if "HUGGINGFACEHUB_API_TOKEN" in st.secrets:
    hf_token = st.secrets["HUGGINGFACEHUB_API_TOKEN"]
    try:
        login(hf_token)
        st.sidebar.success("Logged into Hugging Face (from Streamlit secrets).")
    except Exception as e:
        st.sidebar.error(f"Hugging Face login failed: {e}")
else:
    st.sidebar.warning("No HUGGINGFACEHUB_API_TOKEN in st.secrets. Public model downloads will still work but may be slower / rate-limited.")

st.sidebar.markdown("### Model & settings")

model_choice_key = st.sidebar.radio("Model size", options=list(MODEL_OPTIONS.keys()), index=list(MODEL_OPTIONS.keys()).index(DEFAULT_MODEL))
model_name = MODEL_OPTIONS[model_choice_key]

use_cache = st.sidebar.checkbox("Use answer cache", value=True)
deterministic = st.sidebar.checkbox("Deterministic responses (do_sample=False)", value=True)
max_length = st.sidebar.slider("Max generated tokens (max_length)", min_value=64, max_value=1024, value=256, step=64)
show_advanced = st.sidebar.expander("Advanced / Controls", expanded=False)
with show_advanced:
    st.write("Force reload pipeline or clear caches if needed.")
    if st.button("Force reload model pipeline (clear cached pipeline)"):
        # clear the pipeline resource cache
        try:
            load_pipeline.clear()
        except Exception:
            # if function not yet defined, ignore
            pass
        st.experimental_rerun()
    if st.button("Clear answer cache"):
        try:
            answer_cached.clear()
            st.sidebar.success("Answer cache cleared.")
        except Exception:
            st.sidebar.error("Could not clear cache (maybe not initialized yet).")

# Display model note/warning
if model_choice_key == "large":
    st.sidebar.warning("flan-t5-large is much heavier. It may be slow on CPU, and may time out on free hosts.")

st.sidebar.markdown("---")
st.sidebar.caption("Pipeline auto-refresh: every 5 minutes. Answers cached for 1 hour when caching is enabled.")

# ---------------------------
# Session state for rate limiting
# ---------------------------
if "last_call_time" not in st.session_state:
    st.session_state["last_call_time"] = 0.0
if "call_counter" not in st.session_state:
    st.session_state["call_counter"] = 0

def can_invoke() -> Tuple[bool, float]:
    """Return (allowed_bool, seconds_until_allowed)."""
    now = time.time()
    elapsed = now - st.session_state["last_call_time"]
    if elapsed >= CALL_COOLDOWN_SECONDS:
        return True, 0.0
    else:
        return False, CALL_COOLDOWN_SECONDS - elapsed

# ---------------------------
# CACHED RESOURCE: pipeline (auto reload every PIPELINE_TTL_SECONDS)
# ---------------------------
@st.cache_resource(ttl=PIPELINE_TTL_SECONDS)
def load_pipeline(model_id: str):
    """Load and cache a transformers pipeline for a given model_id. Auto-refresh after TTL."""
    # We rely on huggingface_hub.login earlier (via st.secrets) if provided.
    # Use text2text-generation for FLAN T5
    st.sidebar.info(f"Loading model {model_id} (this may take a few seconds the first time)...")
    pipe = pipeline("text2text-generation", model=model_id)
    # no device specification: use default (CPU on Streamlit Cloud), user can adapt for GPU if available.
    return pipe

# ---------------------------
# CACHED FUNCTION: answer generation
# ---------------------------
# We cache by model + prompt + max_length + deterministic setting. TTL reduces repeated calls.
@st.cache_data(ttl=ANSWER_CACHE_TTL)
def answer_cached(model_id: str, prompt: str, max_length: int, do_sample: bool) -> str:
    """Call pipeline and return generated text. This function result is cached."""
    pipe = load_pipeline(model_id)
    # call the pipeline
    res = pipe(prompt, max_length=max_length, do_sample=do_sample)
    # pipeline returns a list of dicts; take the first
    text = res[0].get("generated_text") if isinstance(res, list) else str(res)
    return text

# ---------------------------
# UI: Tabs (Normal Q&A, PDF Q&A)
# ---------------------------
tab1, tab2 = st.tabs(["ðŸ’¬ Normal Q&A", "ðŸ“„ PDF Q&A"])

# -----------------------
# Tab 1: Normal Q&A
# -----------------------
with tab1:
    st.header("Ask a direct question")
    st.markdown("Type a question below and press **Get Answer**. Rate limits & caching applied to avoid repeated hits.")
    question = st.text_area("Question", height=120, placeholder="e.g. What is the mechanism of action of acetaminophen?")
    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("Get Answer", key="direct_get"):
            allowed, wait = can_invoke()
            if not allowed:
                st.warning(f"Please wait {int(wait)}s before another request to avoid rate limits.")
            elif not question.strip():
                st.warning("Please type a question.")
            else:
                # prepare prompt (simple)
                prompt = f"Question: {question}\nAnswer:"
                # Optionally truncate prompt if too long
                # generate
                with st.spinner("Generating answer..."):
                    do_sample = not deterministic
                    if use_cache:
                        answer = answer_cached(model_name, prompt, max_length, do_sample)
                    else:
                        # bypass cache by calling pipeline directly
                        pipe = load_pipeline(model_name)
                        res = pipe(prompt, max_length=max_length, do_sample=do_sample)
                        answer = res[0].get("generated_text")
                # update last call
                st.session_state["last_call_time"] = time.time()
                st.session_state["call_counter"] += 1
                st.success("Answer:")
                st.write(answer)
    with col2:
        st.markdown("**Session info**")
        last = st.session_state["last_call_time"]
        if last:
            st.caption(f"Last model call: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(last))}")
        else:
            st.caption("No model calls yet this session.")
        st.caption(f"Calls this session: {st.session_state['call_counter']}")
        st.info(f"Model: {model_name}  â€¢  Cooldown: {CALL_COOLDOWN_SECONDS}s  â€¢  Pipeline auto-refresh: {PIPELINE_TTL_SECONDS}s")

# -----------------------
# Tab 2: PDF Q&A
# -----------------------
with tab2:
    st.header("Upload PDFs, ask questions, and download answers PDF")
    st.markdown("You can upload multiple source PDFs (corpus) and either upload a PDF with line-separated questions or type questions.")
    uploaded_pdfs = st.file_uploader("Source PDF(s) (select multiple)", type="pdf", accept_multiple_files=True)
    questions_pdf = st.file_uploader("Questions PDF (optional) â€” one question per new line", type="pdf", key="questions_pdf")
    manual_questions = st.text_area("Or paste/type questions here (one per line)", height=120, placeholder="One question per line. These will be used if provided.")
    truncate_note = st.caption("Context may be truncated to keep prompts within safe size limits.")

    # Button area
    if st.button("Process and Generate Answers PDF", key="process_pdf_btn"):
        allowed, wait = can_invoke()
        if not allowed:
            st.warning(f"Please wait {int(wait)}s before another request to avoid rate limits.")
        elif not uploaded_pdfs:
            st.warning("Please upload at least one source PDF.")
        else:
            with st.spinner("Extracting text from PDFs..."):
                # extract corpus
                corpus_parts: List[str] = []
                for up in uploaded_pdfs:
                    try:
                        reader = PdfReader(up)
                        text = ""
                        for p in reader.pages:
                            text += p.extract_text() or ""
                        corpus_parts.append(text)
                    except Exception as e:
                        st.error(f"Failed to read {getattr(up, 'name', 'uploaded file')}: {e}")
                corpus = "\n\n".join(corpus_parts)

                # extract questions list
                questions: List[str] = []
                if manual_questions and manual_questions.strip():
                    questions = [q.strip() for q in manual_questions.splitlines() if q.strip()]
                elif questions_pdf:
                    try:
                        reader = PdfReader(questions_pdf)
                        qtext = ""
                        for p in reader.pages:
                            qtext += p.extract_text() or ""
                        questions = [q.strip() for q in qtext.splitlines() if q.strip()]
                    except Exception as e:
                        st.error(f"Failed to read questions PDF: {e}")
                else:
                    st.warning("No questions provided. Enter questions or upload a questions PDF.")
                    questions = []

            if not questions:
                st.warning("No valid questions found â€” nothing to do.")
            else:
                # prepare context with a safe truncation depending on model
                max_ctx = CONTEXT_CHAR_LIMIT.get(model_choice_key, 2000)
                context = corpus
                truncated = False
                if len(context) > max_ctx:
                    context = context[:max_ctx]
                    truncated = True

                if truncated:
                    st.warning(f"Source corpus truncated to {max_ctx} characters for prompt safety. Consider using fewer PDFs or a smaller context block if you need full-text answers.")

                # Now generate answers (cached if enabled)
                answers = []
                do_sample = not deterministic
                with st.spinner("Generating answers (this may take time for large models)..."):
                    for q in questions:
                        prompt = f"Context:\n{context}\n\nQuestion: {q}\nAnswer:"
                        if use_cache:
                            ans_text = answer_cached(model_name, prompt, max_length, do_sample)
                        else:
                            pipe = load_pipeline(model_name)
                            res = pipe(prompt, max_length=max_length, do_sample=do_sample)
                            ans_text = res[0].get("generated_text")
                        answers.append((q, ans_text))

                # update last call timestamp + counter
                st.session_state["last_call_time"] = time.time()
                st.session_state["call_counter"] += 1

                # Write answers to PDF
                buffer = io.BytesIO()
                c = canvas.Canvas(buffer, pagesize=letter)
                width, height = letter
                y = height - 50

                c.setFont("Helvetica-Bold", 12)
                c.drawString(50, y, "Generated Answers")
                y -= 30
                c.setFont("Helvetica", 10)

                for q, a in answers:
                    # question
                    c.setFont("Helvetica-Bold", 10)
                    for line in wrap_text(f"Q: {q}", 90):
                        c.drawString(50, y, line)
                        y -= 14
                        if y < 60:
                            c.showPage()
                            y = height - 50
                    y -= 4
                    # answer
                    c.setFont("Helvetica", 10)
                    for line in wrap_text(a, 90):
                        c.drawString(60, y, line)
                        y -= 12
                        if y < 60:
                            c.showPage()
                            y = height - 50
                    y -= 18

                c.save()
                buffer.seek(0)
                st.success("Answers generated!")
                st.download_button("ðŸ“¥ Download answers.pdf", data=buffer.getvalue(), file_name="answers.pdf", mime="application/pdf")


# ---------------------------
# Utilities
# ---------------------------
def wrap_text(text: str, width: int = 80) -> List[str]:
    """
    Simple wrapper to break long text into lines for PDF output.
    """
    import textwrap
    return textwrap.wrap(text, width=width)

# ---------------------------
# FOOTER: show cooldown / next availability
# ---------------------------
allowed, wait = can_invoke()
if not allowed:
    st.info(f"Next free model invocation available in about {int(wait)} seconds.")
else:
    st.caption("You may invoke the model now. Calls will be rate-limited to avoid hitting usage limits.")

st.caption(f"Note: pipelines automatically refresh every {PIPELINE_TTL_SECONDS // 60} minutes to avoid stale resource issues.")